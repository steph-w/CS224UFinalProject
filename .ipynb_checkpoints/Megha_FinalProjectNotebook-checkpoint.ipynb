{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS224U Final Project: Analysis of Gender Roles and Bias in Literary Portrayal of Characters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__authors__ = \"Stephanie Wang, Megha Srivastava, Sarai Gould\"\n",
    "__version__=\"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "import scipy.stats\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "##### Features\n",
    "The input to these functions is a list of features to train on. Ideally this is a list of dictionaries.\n",
    "We are using unigrams as our features, for which we have sparse vector representations for each sentence in the text. We are also using the dependency parses as a feature set. \n",
    "##### Binary Classification\n",
    "We are looking at binary tasks -- classifying as either male/ female\n",
    "##### Train and Test\n",
    "We need to split our data into two different sets -- one for training our classifier, and one for testing the model for accuracy.\n",
    "#### TO DO:\n",
    " - Add a function to read in the features as a tuple -- (dictionary, label) where dictionary is the dictionary for a sentence, and label is \"female\" or \"male\".\n",
    " - Account for the fact that a sentence could have both male and female labels\n",
    " - Add hyperparameter tuning for better results. Try out other classification models for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs \n",
    "\n",
    "#labels are: Male Char, Female Auth: MCFA; Female Char, Female Auth: FCFA; Male Char, Male Auth: MCMA;\n",
    "#Female Char, Male Auth: FCMA; Male Char, Female Char, Female Auth: MCFCFA; Male Char, Female Char Male Auth: MCFCMA\n",
    "def train_file_reader(src_filename):\n",
    "    curr_author_gender = \"FA\"\n",
    "    curr_genre = \"Default_Genre\"\n",
    "    curr_pub = \"0000\"\n",
    "    all_lines_file = codecs.open(src_filename, 'r', 'utf8')\n",
    "    iterable_file = iter(all_lines_file)\n",
    "    for line in iterable_file:\n",
    "        if \"#####\" in line:   \n",
    "            #line represents a new book in the dataset, indicating a change in author gender, pub year, and genre\n",
    "            if \"GENDER:MALE\" in line:\n",
    "                curr_author_gender = \"MA\"\n",
    "            else:\n",
    "                curr_author_gender = \"FA\"\n",
    "            pub_year_pattern = re.compile('#PUB:(\\d+)#') #extracts publication year\n",
    "            pub_matches = pub_year_pattern.findall(line)\n",
    "            if pub_matches:\n",
    "                curr_pub = pub_matches[0]\n",
    "            genre_pattern = re.compile('#GENRE:(.+)#PUB') #extracts genre\n",
    "            genre_matches = genre_pattern.findall(line)\n",
    "            if genre_matches:\n",
    "                curr_genre = genre_matches[0]\n",
    "        if line.startswith(\"SENTENCE: \"): #sentence to label\n",
    "            label = \"\"\n",
    "            if \"00MALE00\" in line:\n",
    "                label += \"MC\"\n",
    "            if \"00FEMALE00\" in line:\n",
    "                label += \"FC\"\n",
    "            if label == \"\":\n",
    "                continue\n",
    "            label += curr_author_gender\n",
    "            unigrams_list = []\n",
    "            for word in line.split(\" \"):\n",
    "                if (word != \"00MALE00\" and word != \"00FEMALE00\" and word != \"SENTENCE:\"):\n",
    "                    unigrams_list.append(word) #gets all unigrams except those obviously indicating gender\n",
    "            dependency_list = []\n",
    "            line = next(iterable_file)\n",
    "            if line.startswith(\"DEPENDENCY: \"): #dependencies corresponding to above sentence\n",
    "                for dep in line.split(\" \"):\n",
    "                    if (dep != \"DEPENDENCY:\"):\n",
    "                        dependency_list.append(dep)\n",
    "            if not dependency_list:\n",
    "                continue\n",
    "            yield(unigrams_list, dependency_list, curr_pub, curr_genre, label) #returns a generator\n",
    "        \n",
    "def train_reader():\n",
    "    #hard-code file HERE\n",
    "    return train_file_reader(\"replaced_all_lines.txt.sentences.extracted.dep\")    \n",
    "\n",
    "def features_phi(unigrams, dependencies, pub, genre):\n",
    "    features_list = []\n",
    "    features_list.extend(unigrams)\n",
    "    features_list.extend(dependencies)\n",
    "    #publication date feature is currently divided into buckets by century. Change this code to \n",
    "    #use buckets of decades, centuries, etc. \n",
    "    if pub.startswith(\"18\"):\n",
    "        features_list.append(\"1800\")\n",
    "    elif pub.startswith(\"19\"):\n",
    "        features_list.append(\"1900\")\n",
    "    elif pub.startswith(\"17\"):\n",
    "        features_list.append(\"1700\")\n",
    "    elif pub.startswith(\"16\"):\n",
    "        features_list.append(\"1600\")\n",
    "    else:\n",
    "        features_list.append(\"0000\")\n",
    "    #appends genre feature\n",
    "    features_list.append(genre)\n",
    "    #turns feature list into a dictionary\n",
    "    return Counter(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(reader, phi, vectorizer=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    reader : iterator\n",
    "        This is the dataset that we are featurizing\n",
    "        \n",
    "    vectorizer : sklearn.feature_extraction.DictVectorizer\n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "\n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of labels), \n",
    "        'vectorizer' (the 'DictVectorizer'), and 'raw_examples'(for error analysis).\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    feat_dicts = []\n",
    "    raw_examples = []\n",
    "    for unigrams, deps, pub, genre, label in reader():\n",
    "        labels.append(label)\n",
    "        feat_dicts.append(phi(unigrams, deps, pub, genre))\n",
    "        raw_examples.append(unigrams)\n",
    "    feat_matrix = None\n",
    "    #In training, we want a new vectorizer:\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "        feat_names = vectorizer.get_feature_names()\n",
    "    #In assessment, we featurize using the existing vectorizer:\n",
    "    else:\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    \n",
    "    return {'X': feat_matrix,\n",
    "            'y': labels,\n",
    "            'featureNames': feat_names,\n",
    "            'vectorizer': vectorizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the classifier on our feature matrix and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.linear.model.LogisticRegression\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment to train and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(\n",
    "            train_reader=train_reader,\n",
    "            assess_reader=None,\n",
    "            train_size=0.7,\n",
    "            phi = features_phi,\n",
    "            train_func=fit_maxent_classifier,\n",
    "            score_func=utils.safe_macro_f1,\n",
    "            verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    train_reader : Iterator for training data.\n",
    "    \n",
    "    assess_reader : Iterator for assessment data.\n",
    "    \n",
    "    train_size : float\n",
    "        If 'assess_reader' is None, then this is percentage of \n",
    "        'train_reader' devoted to training. Else this is ignored.\n",
    "    train_func : model wrapper\n",
    "        Any function taking in a feature matrix and label list\n",
    "        and returns a fitted model with a 'predict' function\n",
    "        that operates on feature matrices.\n",
    "    score_metric : Scoring function, default is weighted average F1\n",
    "    verbose : bool\n",
    "        Whether to print out model assessment to standard output.\n",
    "    \n",
    "    Prints\n",
    "    ======\n",
    "    Model accuracy and model precision/recall/F1 report.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Float\n",
    "        Overall scoring metric determined by 'score_metric'.\n",
    "    \"\"\"\n",
    "    # Train dataset:\n",
    "    train = build_dataset(train_reader, phi, vectorizer=None)\n",
    "    #Manage the assessment set-up:\n",
    "    X_train = train['X']\n",
    "    y_train = train['y']\n",
    "    feature_names = train['featureNames']\n",
    "    X_assess = None\n",
    "    y_assess = None\n",
    "    if assess_reader == None:\n",
    "        #'train_test_split' is a sklearn function that splits arrays or matrices into random train and test subsets\n",
    "        X_train, X_assess, y_train, y_assess = train_test_split(\n",
    "                X_train, y_train, train_size=train_size)\n",
    "    else:\n",
    "        #Assessment dataset using the training vectorizer:\n",
    "        assess = build_dataset(assess_reader, phi, vectorizer=None)\n",
    "        X_assess, y_assess = assess['X'], assess['y']\n",
    "    #Train:\n",
    "    mod = train_func(X_train, y_train)\n",
    "    #Predictions:\n",
    "    predictions = mod.predict(X_assess)\n",
    "    #Print features and feature weights\n",
    "    coef = np.matrix(mod.coef_)\n",
    "    df = DataFrame(coef.transpose(), index = feature_names)\n",
    "    print('Feature names and their weights:')\n",
    "    print (df.sort_values([2], ascending=[False]))\n",
    "    #Report:\n",
    "    if verbose:\n",
    "        print('Accuracy: %0.03f' % accuracy_score(y_assess, predictions))\n",
    "        print(classification_report(y_assess, predictions, digits=3))\n",
    "    # Return the overall score:\n",
    "    return score_func(y_assess, predictions) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names and their weights:\n",
      "                                     0         1         2\n",
      "Mr.                          -0.559809 -0.597556  0.847500\n",
      "himself                      -1.014760 -0.028129  0.820026\n",
      "brother                      -0.537486 -0.274900  0.653254\n",
      "who                          -0.837737 -0.049916  0.640674\n",
      "Africa                       -0.654916 -1.112743  0.612274\n",
      "!                            -0.623025 -0.005867  0.561063\n",
      "compound_hust_00unknown00    -0.372820 -0.249542  0.550899\n",
      "Hust                         -0.372820 -0.249542  0.550899\n",
      "Bramble                      -0.698615  0.002973  0.540497\n",
      "if                           -0.275452 -0.351671  0.535393\n",
      ":                            -0.536173 -0.259557  0.532985\n",
      "St.                          -0.550859 -0.056552  0.532510\n",
      "from                         -0.371317 -0.391054  0.509517\n",
      "whose                        -0.401558 -0.179912  0.504576\n",
      "they                         -0.428538 -0.206128  0.487361\n",
      "1800                         -0.507742 -1.169752  0.486285\n",
      "received                     -0.305711 -0.238033  0.476189\n",
      "-LSB-                        -0.333372 -0.295432  0.447893\n",
      "where                        -0.406804 -0.090156  0.440492\n",
      "heard                        -0.283164 -0.207067  0.426397\n",
      "They                         -0.285442 -0.165991  0.413977\n",
      "sleep                        -0.333116 -0.095557  0.411090\n",
      "say                          -0.456548  0.014713  0.410673\n",
      "How                          -0.298944 -0.163680  0.409842\n",
      "compound_bramble_00unknown00 -0.554866  0.036810  0.407893\n",
      "I                             0.035519 -0.679460  0.402808\n",
      "nsubj_make_00unknown00       -0.274253 -0.131499  0.398459\n",
      "make                         -0.195227 -0.303952  0.387230\n",
      "det_king_the                 -0.294183 -0.174847  0.381885\n",
      "here                         -0.483949  0.049059  0.379258\n",
      "...                                ...       ...       ...\n",
      "advmod_unhappy_very           0.534838 -0.040459 -0.488374\n",
      "began                         0.541743 -0.027090 -0.488983\n",
      "been                          0.818212 -0.315925 -0.495979\n",
      "Miss                          0.384048  0.181361 -0.497646\n",
      "case_00unknown00_of          -0.358730  1.011082 -0.502534\n",
      "almost                        0.583650 -0.036614 -0.509688\n",
      "half                          0.250063  0.313202 -0.512412\n",
      "ship                          0.216453  0.467490 -0.533464\n",
      "sight                         0.444698  0.123401 -0.537697\n",
      "nsubj_thought_00unknown00     0.576253  0.033092 -0.540481\n",
      "dobj_replied_00unknown00\\n    0.583741 -0.027033 -0.545712\n",
      "abbey                         0.473060  0.112073 -0.549914\n",
      "window                        0.563812  0.030283 -0.551943\n",
      "home                          0.345834  0.240711 -0.554469\n",
      "compound_soldier_00unknown00 -0.115673  0.805341 -0.561783\n",
      "beautiful                     0.425616  0.175680 -0.562062\n",
      "det_00unknown00_this          0.589694  0.052648 -0.569887\n",
      "nsubj_had_00unknown00         0.444471  0.254239 -0.611126\n",
      "looked                        0.101616  0.622169 -0.655513\n",
      "root_root_answered            0.464752  0.310418 -0.663701\n",
      "daughter                      0.433258  0.286547 -0.667584\n",
      "thought                       0.647252  0.125132 -0.719892\n",
      "nmod:poss_mother_00unknown00  0.577968  0.285429 -0.746878\n",
      "compound_maret_00unknown00    0.733342  0.008874 -0.762827\n",
      "Maret                         0.733342  0.008874 -0.762827\n",
      "cc_00unknown00_and            0.160864  0.712190 -0.800184\n",
      "Huntington                    0.387226  0.490815 -0.842612\n",
      "Maud                          0.527401  0.524288 -0.932384\n",
      "herself                       0.967666 -0.000652 -0.961345\n",
      "mother                        0.625236  0.568346 -1.042213\n",
      "\n",
      "[82768 rows x 3 columns]\n",
      "Accuracy: 0.744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FCMA      0.783     0.138     0.235       130\n",
      "     MCFCMA      0.580     0.287     0.384       101\n",
      "       MCMA      0.755     0.978     0.852       536\n",
      "\n",
      "avg / total      0.737     0.744     0.686       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid, scoring='accuracy'):\n",
    "    #Find the best model within param_grid:\n",
    "    crossvalidator = GridSearchCV(basemod, param_grid, cv=cv, scoring=scoring)\n",
    "    crossvalidator.fit(X, y)\n",
    "    print(\"Best params\", crossvalidator.best_params_)\n",
    "    print(\"Best score: %0.03f\" % crossvalidator.best_score_)\n",
    "    return crossvalidator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_with_crossvalidation(X, y):\n",
    "    basemod = LogisticRegression()\n",
    "    cv = 5\n",
    "    param_grid = {'fit_intercept': [True, False],\n",
    "                 'C': [0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 0.9, 0.5],\n",
    "                 'penalty': ['l1', 'l2']}\n",
    "    return fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best params', {'penalty': 'l1', 'C': 0.6, 'fit_intercept': False})\n",
      "Best score: 0.741\n",
      "Feature names and their weights:\n",
      "                                      0         1         2\n",
      "Corrard                        0.000000 -0.862152  1.085090\n",
      "himself                       -1.638061 -0.359759  1.007214\n",
      "heard                         -0.106515 -0.175148  0.986680\n",
      "king                          -0.088170  0.000000  0.961549\n",
      "guide                          0.000000 -0.341831  0.955942\n",
      "servant                        0.000000 -0.094716  0.947406\n",
      "if                            -0.193304 -0.651099  0.890419\n",
      "received                      -0.692499  0.000000  0.864009\n",
      "root_root_00unknown00         -0.213864  0.000000  0.863680\n",
      "1800                           0.000000 -2.918631  0.840286\n",
      "compound_00unknown00_st.       0.000000  0.000000  0.839233\n",
      "Hust                          -0.353934 -0.208110  0.819039\n",
      "orders                         0.000000 -0.289849  0.800969\n",
      "n't                           -0.290470 -0.450930  0.769904\n",
      ":                             -0.721509 -0.357460  0.705032\n",
      "whose                         -0.840027  0.000000  0.696177\n",
      "old                           -0.060701  0.000000  0.668878\n",
      "say                           -0.359088  0.000000  0.663850\n",
      "-LSB-                         -0.256073  0.000000  0.580513\n",
      "arrived                        0.000000  0.000000  0.567949\n",
      "?                              0.000000 -0.625738  0.526100\n",
      "engaged                        0.000000  0.000000  0.502883\n",
      "nsubj_make_00unknown00        -0.571709  0.000000  0.494797\n",
      "should                        -0.246054 -0.136100  0.480592\n",
      "Africa                        -1.284300  0.000000  0.470747\n",
      "also                           0.000000 -0.093323  0.458633\n",
      "natural                        0.000000  0.000000  0.446022\n",
      "near                           0.000000 -0.099613  0.442708\n",
      "!                             -0.902453  0.000000  0.437001\n",
      "side                          -0.634778  0.000000  0.435690\n",
      "...                                 ...       ...       ...\n",
      "beautiful                      0.559679  0.000000 -0.820887\n",
      "change                         0.140510  0.000000 -0.872461\n",
      "until                         -0.138991  1.244396 -0.893955\n",
      "amod_commander_young           0.000000  0.572840 -0.903545\n",
      "anchor                         0.670390  0.000000 -0.911175\n",
      "det_men_the                    1.542004  0.000000 -0.929961\n",
      "May                            0.238433  0.698272 -0.937947\n",
      "dep_river_mr.                  0.000000  0.105017 -0.980917\n",
      "Hetty                          0.877331  0.000000 -0.983797\n",
      "father                         0.000000  1.152673 -1.004569\n",
      "result                         1.155648  0.000000 -1.034144\n",
      "root_root_answered             0.040347  0.000000 -1.049247\n",
      "sight                          0.808659  0.000000 -1.078709\n",
      "charms                         0.000000  0.000000 -1.085850\n",
      "nsubj_hastened_00unknown00     0.000000  0.219407 -1.089942\n",
      "det_00unknown00_the            0.579742  0.852697 -1.143128\n",
      "frank                          0.000000  0.703289 -1.191453\n",
      "amod_00unknown00_poor          0.000000  0.462414 -1.223844\n",
      "childish                       0.000000  0.000000 -1.228989\n",
      "began                          0.872654  0.000000 -1.233792\n",
      "det_vessel_the                 0.000000  0.295382 -1.276562\n",
      "sat                            0.000000  0.275700 -1.282345\n",
      "queen                          0.000000  0.000000 -1.386724\n",
      "whole                          0.000000  0.511031 -1.482850\n",
      "named                          0.660863  0.000000 -1.553993\n",
      "nmod:poss_husband_00unknown00  0.000000  0.508342 -1.564384\n",
      "compound_maret_00unknown00     2.320237  0.000000 -1.733273\n",
      "Huntington                     0.591751  0.559020 -1.928665\n",
      "mother                         0.623125  0.570238 -1.986754\n",
      "herself                        1.509912  0.753594 -3.105789\n",
      "\n",
      "[82768 rows x 3 columns]\n",
      "Accuracy: 0.738\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FCMA      0.500     0.258     0.340       124\n",
      "     MCFCMA      0.500     0.257     0.340       101\n",
      "       MCMA      0.780     0.937     0.852       542\n",
      "\n",
      "avg / total      0.698     0.738     0.702       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = experiment(\n",
    "        train_func=fit_maxent_with_crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
