{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS224U Final Project: Analysis of Gender Roles and Bias in Literary Portrayal of Characters\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__authors__ = \"Stephanie Wang, Megha Srivastava, Sarai Gould\"\n",
    "__version__=\"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from pandas import DataFrame\n",
    "import scipy.stats\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "##### Features\n",
    "The input to these functions is a list of features to train on. Ideally this is a list of dictionaries which can then be dictvectorized into a feature matrix. \n",
    "Current features:\n",
    " - Unigrams\n",
    " - Dependencies\n",
    " - Genres\n",
    " - Date of Publication\n",
    " - Sentence Length\n",
    "\n",
    "##### Binary Classification\n",
    "We are looking at binary tasks -- classifying as either male/ female\n",
    "##### Train and Test\n",
    "We need to split our data into two different sets -- one for training our classifier, and one for testing the model for accuracy. We use Hyperparameter search to choose the best parameters for our model -- this works best on certain datasets but may not produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs \n",
    "\n",
    "#labels are: Male Char, Female Auth: MCFA; Female Char, Female Auth: FCFA; Male Char, Male Auth: MCMA;\n",
    "#Female Char, Male Auth: FCMA; Male Char, Female Char, Female Auth: MCFCFA; Male Char, Female Char Male Auth: MCFCMA\n",
    "def train_file_reader(src_filename):\n",
    "    curr_author_gender = \"FA\"\n",
    "    curr_genre = \"Default_Genre\"\n",
    "    curr_pub = \"0000\"\n",
    "    all_lines_file = codecs.open(src_filename, 'r', 'utf8')\n",
    "    iterable_file = iter(all_lines_file)\n",
    "    for line in iterable_file:\n",
    "        if \"#####\" in line:   \n",
    "            #line represents a new book in the dataset, indicating a change in author gender, pub year, and genre\n",
    "            if \"GENDER:MALE\" in line:\n",
    "                curr_author_gender = \"MA\"\n",
    "            else:\n",
    "                curr_author_gender = \"FA\"\n",
    "            pub_year_pattern = re.compile('#PUB:(\\d+)#') #extracts publication year\n",
    "            pub_matches = pub_year_pattern.findall(line)\n",
    "            if pub_matches:\n",
    "                curr_pub = pub_matches[0]\n",
    "            genre_pattern = re.compile('#GENRE:(.+)#PUB') #extracts genre\n",
    "            genre_matches = genre_pattern.findall(line)\n",
    "            if genre_matches:\n",
    "                curr_genre = genre_matches[0]\n",
    "        if line.startswith(\"SENTENCE: \"): #sentence to label\n",
    "            label = \"\"\n",
    "            if \"00MALE00\" in line:\n",
    "                label += \"MC\"\n",
    "            if \"00FEMALE00\" in line:\n",
    "                label += \"FC\"\n",
    "            if label == \"\":\n",
    "                continue\n",
    "            label += curr_author_gender\n",
    "            unigrams_list = []\n",
    "            for word in line.split(\" \"):\n",
    "                if (word != \"00MALE00\" and word != \"00FEMALE00\" and word != \"SENTENCE:\"):\n",
    "                    unigrams_list.append(word) #gets all unigrams except those obviously indicating gender\n",
    "            unigrams_length = len(unigrams_list) #gets the length of sentence\n",
    "            dependency_list = []\n",
    "            line = next(iterable_file)\n",
    "            if line.startswith(\"DEPENDENCY: \"): #dependencies corresponding to above sentence\n",
    "                for dep in line.split(\" \"):\n",
    "                    if (dep != \"DEPENDENCY:\"):\n",
    "                        dependency_list.append(dep)\n",
    "            if not dependency_list:\n",
    "                continue\n",
    "            yield(unigrams_list, unigrams_length, dependency_list, curr_pub, curr_genre, label) #returns a generator\n",
    "        \n",
    "def train_reader():\n",
    "    #hard-code file HERE\n",
    "    return train_file_reader(\"replaced_all_lines.txt.sentences.extracted.dep\")    \n",
    "\n",
    "def features_phi(unigrams, length, dependencies, pub, genre):\n",
    "    features_list = []\n",
    "    features_list.extend(unigrams)\n",
    "    features_list.extend(dependencies)\n",
    "    features_list.append(length)\n",
    "    #publication date feature is currently divided into buckets by century. Change this code to \n",
    "    #use buckets of decades, centuries, etc. \n",
    "    if pub.startswith(\"18\"):\n",
    "        features_list.append(\"1800\")\n",
    "    elif pub.startswith(\"19\"):\n",
    "        features_list.append(\"1900\")\n",
    "    elif pub.startswith(\"17\"):\n",
    "        features_list.append(\"1700\")\n",
    "    elif pub.startswith(\"16\"):\n",
    "        features_list.append(\"1600\")\n",
    "    else:\n",
    "        features_list.append(\"0000\")\n",
    "    #appends genre feature\n",
    "    features_list.append(genre)\n",
    "    #turns feature list into a dictionary\n",
    "    return Counter(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(reader, phi, vectorizer=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    reader : iterator\n",
    "        This is the dataset that we are featurizing\n",
    "        \n",
    "    vectorizer : sklearn.feature_extraction.DictVectorizer\n",
    "        If this is None, then a new `DictVectorizer` is created and\n",
    "        used to turn the list of dicts created by `phi` into a \n",
    "        feature matrix. This happens when we are training.\n",
    "\n",
    "        If this is not None, then it's assumed to be a `DictVectorizer` \n",
    "        and used to transform the list of dicts. This happens in \n",
    "        assessment, when we take in new instances and need to \n",
    "        featurize them as we did in training.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict with keys 'X' (the feature matrix), 'y' (the list of labels), \n",
    "        'vectorizer' (the 'DictVectorizer'), and 'raw_examples'(for error analysis).\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    feat_dicts = []\n",
    "    for unigrams, length, deps, pub, genre, label in reader():\n",
    "        labels.append(label)\n",
    "        feat_dicts.append(phi(unigrams, length, deps, pub, genre))\n",
    "    feat_matrix = None\n",
    "    #In training, we want a new vectorizer:\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=True)\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "        feat_names = vectorizer.get_feature_names()\n",
    "    #In assessment, we featurize using the existing vectorizer:\n",
    "    else:\n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "    \n",
    "    return {'X': feat_matrix,\n",
    "            'y': labels,\n",
    "            'featureNames': feat_names,\n",
    "            'vectorizer': vectorizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the classifier on our feature matrix and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y):    \n",
    "    \"\"\"Wrapper for `sklearn.linear.model.LogisticRegression`. This is also \n",
    "    called a Maximum Entropy (MaxEnt) Classifier, which is more fitting \n",
    "    for the multiclass case.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d np.array\n",
    "        The matrix of features, one example per row.\n",
    "        \n",
    "    y : list\n",
    "        The list of labels for rows in `X`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sklearn.linear.model.LogisticRegression\n",
    "        A trained `LogisticRegression` instance.\n",
    "    \n",
    "    \"\"\"\n",
    "    mod = LogisticRegression(fit_intercept=True)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment to train and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(\n",
    "            train_reader=train_reader,\n",
    "            assess_reader=None,\n",
    "            train_size=0.7,\n",
    "            phi = features_phi,\n",
    "            train_func=fit_maxent_classifier,\n",
    "            score_func=utils.safe_macro_f1,\n",
    "            verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    train_reader : Iterator for training data.\n",
    "    \n",
    "    assess_reader : Iterator for assessment data.\n",
    "    \n",
    "    train_size : float\n",
    "        If 'assess_reader' is None, then this is percentage of \n",
    "        'train_reader' devoted to training. Else this is ignored.\n",
    "    train_func : model wrapper\n",
    "        Any function taking in a feature matrix and label list\n",
    "        and returns a fitted model with a 'predict' function\n",
    "        that operates on feature matrices.\n",
    "    score_metric : Scoring function, default is weighted average F1\n",
    "    verbose : bool\n",
    "        Whether to print out model assessment to standard output.\n",
    "    \n",
    "    Prints\n",
    "    ======\n",
    "    Model accuracy and model precision/recall/F1 report.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Float\n",
    "        Overall scoring metric determined by 'score_metric'.\n",
    "    \"\"\"\n",
    "    # Train dataset:\n",
    "    train = build_dataset(train_reader, phi, vectorizer=None)\n",
    "    #Manage the assessment set-up:\n",
    "    X_train = train['X']\n",
    "    y_train = train['y']\n",
    "    feature_names = train['featureNames']\n",
    "    X_assess = None\n",
    "    y_assess = None\n",
    "    if assess_reader == None:\n",
    "        #'train_test_split' is a sklearn function that splits arrays or matrices into random train and test subsets\n",
    "        X_train, X_assess, y_train, y_assess = train_test_split(\n",
    "                X_train, y_train, train_size=train_size)\n",
    "    else:\n",
    "        #Assessment dataset using the training vectorizer:\n",
    "        assess = build_dataset(assess_reader, phi, vectorizer=None)\n",
    "        X_assess, y_assess = assess['X'], assess['y']\n",
    "    #Train:\n",
    "    mod = train_func(X_train, y_train)\n",
    "    #Predictions:\n",
    "    predictions = mod.predict(X_assess)\n",
    "    #Print features and feature weights\n",
    "    coef = np.matrix(mod.coef_)\n",
    "    df = DataFrame(coef.transpose(), index = feature_names)\n",
    "    print('Feature names and their weights:')\n",
    "    print (df.sort_values([2], ascending=[False]))\n",
    "    #Report:\n",
    "    if verbose:\n",
    "        print('Accuracy: %0.03f' % accuracy_score(y_assess, predictions))\n",
    "        print(classification_report(y_assess, predictions, digits=3))\n",
    "    # Return the overall score:\n",
    "    return score_func(y_assess, predictions) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names and their weights:\n",
      "                                     0         1         2\n",
      "himself                      -0.892743 -0.190090  0.830168\n",
      "Africa                       -0.641856 -1.095129  0.615847\n",
      "who                          -0.692571 -0.045882  0.577640\n",
      "compound_hust_00unknown00    -0.374375 -0.255036  0.551353\n",
      "Hust                         -0.374375 -0.255036  0.551353\n",
      "king                         -0.421220 -0.215322  0.537894\n",
      "from                         -0.444372 -0.311644  0.536534\n",
      "whose                        -0.412540 -0.209706  0.528382\n",
      "prisoner                     -0.307278 -0.236012  0.516859\n",
      "country                      -0.394303 -0.208561  0.507488\n",
      "Bramble                      -0.728138  0.128595  0.499109\n",
      "if                           -0.277791 -0.324162  0.492098\n",
      "St.                          -0.448252 -0.112906  0.490764\n",
      "det_king_the                 -0.421499 -0.166330  0.467745\n",
      "1800                         -0.508647 -1.123657  0.449734\n",
      "root_root_00unknown00        -0.431671 -0.097322  0.447854\n",
      "A                            -0.216894 -0.301251  0.441206\n",
      "they                         -0.424511 -0.176747  0.435813\n",
      "nothing                      -0.193599 -0.282744  0.432774\n",
      "here                         -0.467139 -0.009536  0.428696\n",
      ":                            -0.457153 -0.195830  0.427487\n",
      "?                            -0.116365 -0.461533  0.421729\n",
      "n't                          -0.210471 -0.295789  0.421608\n",
      "next                         -0.316880 -0.118165  0.420054\n",
      "Corrard                      -0.225256 -0.302228  0.408021\n",
      "fellow                       -0.272490 -0.185665  0.403075\n",
      "brother                      -0.385957 -0.070339  0.400802\n",
      "our                          -0.153279 -0.383797  0.395837\n",
      "36                           -0.165054 -0.255442  0.395318\n",
      "Mr.                          -0.599766 -0.148932  0.394282\n",
      "...                                ...       ...       ...\n",
      "sight                         0.345283  0.208051 -0.504109\n",
      "sails                         0.546014 -0.102693 -0.507840\n",
      "Mrs.                          0.442798  0.095032 -0.517300\n",
      "compound_huntington_mrs.      0.442798  0.095032 -0.517300\n",
      "loved                        -0.243209  0.715547 -0.519697\n",
      "nmod:poss_father_00unknown00  0.148514  0.423447 -0.531192\n",
      "root_root_had                 0.422571  0.188911 -0.553602\n",
      "poor                          0.108319  0.498423 -0.555642\n",
      "ship                          0.206530  0.514348 -0.557169\n",
      "May                           0.149028  0.467970 -0.562817\n",
      "root_root_said                0.560760  0.103964 -0.564524\n",
      "Hetty                         0.312862  0.298767 -0.564869\n",
      "been                          0.825731 -0.240331 -0.565211\n",
      "thought                       0.362872  0.285120 -0.582732\n",
      "''                            0.254699  0.584113 -0.589496\n",
      "little                        0.554525  0.113508 -0.608008\n",
      "nmod:poss_mother_00unknown00  0.608809  0.118972 -0.637618\n",
      "det_00unknown00_this          0.512952  0.191386 -0.638099\n",
      "beautiful                     0.439891  0.247921 -0.671939\n",
      "father                        0.074840  0.740060 -0.672314\n",
      "daughter                      0.595786  0.185237 -0.672745\n",
      "But                           0.594496  0.108056 -0.689212\n",
      "Maud                          0.526609  0.404123 -0.774709\n",
      "half                          0.292744  0.645640 -0.823661\n",
      "Maret                         0.790811  0.023099 -0.832537\n",
      "compound_maret_00unknown00    0.790811  0.023099 -0.832537\n",
      "Huntington                    0.648685  0.200950 -0.836561\n",
      "det_00unknown00_the           0.497140  0.551858 -0.865882\n",
      "mother                        0.792468  0.282803 -0.987948\n",
      "herself                       0.792503  0.256655 -0.990482\n",
      "\n",
      "[82921 rows x 3 columns]\n",
      "Accuracy: 0.752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FCMA      0.595     0.188     0.286       117\n",
      "     MCFCMA      0.655     0.194     0.299        98\n",
      "       MCMA      0.765     0.971     0.856       552\n",
      "\n",
      "avg / total      0.725     0.752     0.698       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid, scoring='accuracy'):\n",
    "    #Find the best model within param_grid:\n",
    "    crossvalidator = GridSearchCV(basemod, param_grid, cv=cv, scoring=scoring)\n",
    "    crossvalidator.fit(X, y)\n",
    "    print(\"Best params\", crossvalidator.best_params_)\n",
    "    print(\"Best score: %0.03f\" % crossvalidator.best_score_)\n",
    "    return crossvalidator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_maxent_with_crossvalidation(X, y):\n",
    "    basemod = LogisticRegression()\n",
    "    cv = 5\n",
    "    param_grid = {'fit_intercept': [True, False],\n",
    "                 'C': [0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 0.9, 0.5],\n",
    "                 'penalty': ['l1', 'l2']}\n",
    "    return fit_classifier_with_crossvalidation(X, y, basemod, cv, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best params', {'penalty': 'l2', 'C': 3.0, 'fit_intercept': True})\n",
      "Best score: 0.746\n",
      "Feature names and their weights:\n",
      "                                     0         1         2\n",
      "himself                      -1.109411 -0.306754  1.161283\n",
      "if                           -0.762676 -0.329094  0.947199\n",
      "9                            -0.711355 -0.204440  0.856479\n",
      "Hust                         -0.550109 -0.352546  0.788036\n",
      "compound_hust_00unknown00    -0.550109 -0.352546  0.788036\n",
      "?                            -0.456754 -0.555382  0.762140\n",
      ":                            -0.701785 -0.322369  0.749774\n",
      "Africa                       -0.830435 -1.284667  0.706921\n",
      "Hal                          -0.395119 -0.371450  0.696867\n",
      "who                          -1.008737  0.172454  0.661789\n",
      "St.                          -0.645460 -0.099851  0.651146\n",
      "master                       -0.445963 -0.227338  0.639277\n",
      "from                         -0.481853 -0.407804  0.634360\n",
      "compound_bramble_00unknown00 -0.627601 -0.143824  0.621903\n",
      "11                           -0.315957 -0.448924  0.621859\n",
      "!                            -0.686443  0.005681  0.621111\n",
      "1800                         -0.644530 -1.434314  0.616501\n",
      "CORRY                        -0.593377 -0.090559  0.602834\n",
      "root_root_corry\\n            -0.593377 -0.090559  0.602834\n",
      "cried                        -0.513391 -0.100566  0.596437\n",
      "root_root_is                 -0.585856 -0.005019  0.590432\n",
      "root_root_gave               -0.361799 -0.228652  0.572476\n",
      "orders                       -0.258809 -0.331002  0.561962\n",
      "13                           -0.640611 -0.004188  0.559392\n",
      "nsubj_make_00unknown00       -0.391124 -0.170008  0.549584\n",
      "Bramble                      -0.707580 -0.013866  0.548367\n",
      "therefore                    -0.361764 -0.241210  0.538936\n",
      "remember                     -0.511892 -0.104399  0.527143\n",
      "never                        -0.475903 -0.105650  0.525451\n",
      "death                        -0.428006 -0.156625  0.516530\n",
      "...                                ...       ...       ...\n",
      "loved                        -0.283147  0.956169 -0.707576\n",
      "only                          0.728023  0.030617 -0.709715\n",
      "case_00unknown00_to          -0.498298  1.241344 -0.713214\n",
      "whom                         -0.151428  0.889226 -0.714777\n",
      "case_00unknown00_from        -0.003955  0.735225 -0.727549\n",
      "2                             0.846371 -0.182152 -0.735775\n",
      "father                        0.104225  0.848911 -0.746105\n",
      "As                            0.823397 -0.038117 -0.767370\n",
      "little                        0.673322  0.147732 -0.786346\n",
      "root_root_had                 0.636546  0.299620 -0.797890\n",
      "been                          0.998857 -0.154305 -0.803370\n",
      "But                           0.576338  0.319393 -0.808875\n",
      "nsubj_thought_00unknown00     0.729225  0.187342 -0.809899\n",
      "case_00unknown00_of          -0.117287  1.098412 -0.812118\n",
      "daughter                      0.629561  0.267519 -0.827012\n",
      "nmod:poss_mother_00unknown00  1.027525 -0.060933 -0.852169\n",
      "Maud                          0.501469  0.479366 -0.875855\n",
      "poor                          0.431508  0.585190 -0.890391\n",
      "amod_00unknown00_poor         0.544579  0.502139 -0.918049\n",
      "Who                           0.882937 -0.035603 -0.949977\n",
      "root_root_who                 0.882937 -0.035603 -0.949977\n",
      "half                          0.798301  0.203650 -0.962663\n",
      "det_00unknown00_the           1.054223  0.061911 -0.988673\n",
      "dobj_replied_00unknown00\\n    1.050916 -0.025736 -1.044465\n",
      "thought                       0.957468  0.184089 -1.059547\n",
      "compound_maret_00unknown00    1.068207 -0.062516 -1.080794\n",
      "Maret                         1.068207 -0.062516 -1.080794\n",
      "mother                        0.981304  0.257749 -1.143564\n",
      "Huntington                    0.583947  0.611082 -1.175061\n",
      "herself                       1.178053  0.305714 -1.431972\n",
      "\n",
      "[82921 rows x 3 columns]\n",
      "Accuracy: 0.733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FCMA      0.600     0.236     0.339       127\n",
      "     MCFCMA      0.564     0.210     0.306       105\n",
      "       MCMA      0.752     0.953     0.841       535\n",
      "\n",
      "avg / total      0.701     0.733     0.684       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = experiment(\n",
    "        train_func=fit_maxent_with_crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
